# Coding Theory

KL divergence and entropy are deeply connected to coding theory. It would be distasteful not to say anything about the connection. But it's also not so relevant to the main story, hence this bonus chapter. 

<Expand headline = "Code examples">
Say you've got a long DNA string built from letters \{A,C,G,T\}. You want to store it using as little disk space as possible.  

To understand this question, it's very helpful to make our life easier by ignoring correlations between letters. What I mean by that is that we will achieve the same compression factor for the string 'AAAACCCCGGGGTTTT' as for the string 'ACACTATTCGGCGAGT' - both strings contain the same letters and the first one has some additional structure, but we will not try to exploit it. Technically speaking, we assume that there's some distribution $p$ over the alphabet, and our string comes from sampling repeatedly and independently from $p$.  

If this distribution $p$ is uniform over \{A,C,G,T\}, the best encoding is to use two bits per letter: <Math math = "A \rightarrow \textsf{00}, C \rightarrow \textsf{01}, G \rightarrow \textsf{10}, T \rightarrow \textsf{11} " />. An $N$-letter string takes $2N$ bits. Done! 

But what if $p$ is skewed? Say A shows up half the time in our string, C a quarter, and G,T each an eighth. Now we can do better: <Math math = "A \rightarrow \textsf{0}, C \rightarrow \textsf{10}, G \rightarrow \textsf{110}, T \rightarrow \textsf{111} " />. This only uses 
<Math displayMode={true} id = "code-example" math = "\left( \frac12 \cdot 1 + \frac14 \cdot 2 + \frac18 \cdot 3 + \frac18 \cdot 3 \right) \cdot N = 1.75N" />
bits on average. Cool! 

![encoding](02-crossentropy/encoding.png)

Sure, giving frequent letters shorter codes makes sense, but what's the best way to do it? Coding theory says: __try to give a letter $i$ with frequency $p_i$ a code-name of length $\log 1/p_i$__. 

For example, looking at <EqRef id = "code-example"/>, you can rewrite it like this: 
<Math displayMode={true} id = "code-example2" math = "\left( \frac12 \cdot \log \frac{1}{1/2} + \frac14 \cdot \log \frac{1}{1/4} + \frac18 \cdot \log \frac{1}{1/8} + \frac18 \cdot \log\frac{1}{1/8} \right) \cdot N = 1.75N" />
Every letter with frequency $p$ got code-name of length exactly $\log 1/p$! Notice that in general, the length of codes satisfying the $p \rightarrow \log 1/p$ rule is this: 
<Math displayMode={true} math = "N \cdot \sum_{i = 1}^k p_i \cdot \log \frac{1}{p_i} = N \cdot H(p)." />
That is, if you manage to construct a code with $p \rightarrow \log 1/p$, you spend $H(p)$ bits per letter on average. 

One of the most beautiful and important results of coding theory is that you can always construct a code that spends close to $H(p)$ bits per letter on average, and there's no better code (if we treat letters independently). Details in the next expand box. 
</Expand>

<Expand headline = "Code construction details" advanced = {true}>

Let's see
1. Why we can construct codes with average $H(p)$ bits per letter
2. Why we can't do better (if the letters are independent)

### Construction of a good code <a id = "construction"></a>
First, given a distribution of letter frequencies $p_1, \dots, p_k$, I will show you a construction that's called the [Shannon's code](https://en.wikipedia.org/wiki/Shannon_coding). I will assume that $p_1 \ge p_2 \ge \dots \ge p_k$. 

The first step is that we "round" each probability down to be a power of two. For example, the frequency of the letter 'e' is 12.7% which is between $\frac{1}{2^3} = 12.5\%$ and $\frac{1}{2^2} = 25\%$. The number 3 is important - we will assign a binary code of length 3 to e. Which one? We will assign the codes "greedily" as shown in the following widget. The widget starts with the real English letter frequencies, but you can play with it!

<ShannonCodeWidget />

The problem of Shannon's code is that it only achieves the desired compression of average $H(p)$ bits per letter only if all probabilities are power of two. Only then are all the expressions $\log 1/p_i$ integers and we can make code names of exactly that length. Otherwise, "rounding probabilities down" hurts us. If you do the math, the possibility of dividing each probability by up to 2 in rounding translates to make the compression rate of Shannon's code be up to 1 bit worse that the optimum. That is, you can check in above widget that Average Code Length is always at most 1 bit worse than entropy. 

Here's how we can get rid of this slack of 1 bit. We can construct Shannon's code not for every letter, but for every _pair of letters_. In the case of English, our alphabet would then have size $26^2$. The good thing is that if we construct the Shannon's code for this larger alphabet and lose up to $1$ bit per encoded letter-pair, this means we are only losing $1/2$ bits per actual letter! Doing this trick not for pair of letters but a tuple of several letters can bring us arbitrarily close to a code that spends $H(p)$ bits per letter. So, there are codes that get arbitrarily close to $H(p)$ bits per letter, though they are a bit less intuitive since they are encoding tuples, not individual letters. 

### Why we can't do better

To see why we can't do better than $H(p)$ bits per letter on average, we will have to understand [Kraft's inequlaity](https://en.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality). This inequality says that if we give our $k$ letters code words of lengths $\ell_1, \dots, \ell_k$, then it has to be the case that 
<Math id = "kraft" displayMode = {true} math = "\sum_{i = 1}^k 2^{- \ell_i} \le 1. " />

To understand this inequality, look at the widget above. Whenever we give a code-name to a letter, like $e \rightarrow 000$, we can no longer use codewords that start with '000' for other letters, since that would create clashes. The node with '000' has to be a _leaf_. Some leaves 'take more space' than others - for example, using code name of '0' intuitively kills off half of the space of possibilities. 

A bit more formally, imagine continuing the full binary tree up to some very large depth $N$. Then a code word of length $\ell_i$ is above <Math math="2^{N - \ell_i}" /> of nodes at depth $N$. Since different code words cover disjoint intervals of depth-$N$ nodes, and there are $2^N$ nodes at depth $N$, it has to be the case that 
<Math displayMode={true} math="\sum_{i = 1}^k 2^{N - \ell_i} \le 2^N" />
Divide by $2^N$ and you get Kraft's inequality <EqRef id = "kraft"/>. 

I actually like to think about Kraft's inequality as equality. Why? Well, if your code is such that the left-hand side is really smaller than $1$, then you are stupid. <Footnote>You can notice that Shannon's code above is stupid and leaves some space at the right. In fact, Shannon's code is useful mostly for didactical reasons and in practice you would construct the code by [Huffman's algorithm](https://en.wikipedia.org/wiki/Huffman_coding). </Footnote> In the widget below, you can see how codes can be iteratively improved to get equality. 

<KraftInequalityWidget />


The reason why this is helpful is that I like to think about the numbers <Math math="q_i = 2^{-\ell_i}" /> as some kind of idealized probabilities. Widget above shows that we can pretty much assume that the numbers $q_i$ always sum up to 1. You can think about the numbers $q_i$ as the probability distribution _implied_ by your code-name lengths $\ell_i$. Intuitively, the code is optimized for the distribution $q$, not $p$. 

This setup with $p$ and $q$ is little bit like our discussions of surprises. In fact, let's write down the fact that KL divergence between $p$ and $q$ is always nonnegative. Remember, KL is the difference between crossentropy and entropy, so this is equivalently saying that $H(p, q) \ge H(p)$. If we write the formulas down:
<Math displayMode = {true} math="\sum_{i = 1}^k p_i \log \frac{1}{q_i} \ge \sum_{i = 1}^k p_i \log \frac{1}{p_i}" />
and plug in <Math math="q_i = 2^{-\ell_i}" />, we get
<Math displayMode = {true} math="\sum_{i = 1}^k p_i \ell_i \ge H(p)." />
That is, the average code-name length is at least $H(p)$. In yet another words, no code can beat $H(p)$ bits on average as we wanted to observe. 
</Expand>

The tl;dr is that there is another way to think about entropy. Remember how we defined it as the "average surprisal" of a distribution, say a coin flip? We could also ask: how many bits do we need to store coin flip results? 

If we are flipping a fair coin and want to keep the results, there is nothing smarter to do then remember a file like '0101100' where 0/1 corresponds to H/T. This is 1 bit per flip. 

But if we are flipping a coin where heads have only $0.01$ probability, there are better ways to store the results! For example, we could always store how many Ts there have been between each two Hs. Coding theory formalizes that entropy stands for how much space we need to store the results if we used the best possible encoding. The entropy of the coin flips is "how much info there is to store". 

With coding theory intuitions, cross-entropy also becomes also very natural: it's how many bits you need when data comes from $p$ but you are using the code that's optimized for a different distribution $q$. And relative entropy is how much worse your mismatched code is compared to the optimal one. 

We can now revisit the riddle about how much Wikipedia (and other texts) can be compressed. 

<Expand headline = "ðŸŒ How large is Wikipedia?"><a id = "hutter"></a>
Back to [our Wikipedia riddle](00-introduction#wikipedia) and Hutter's compression challenge. 
Remember, the riddle is about how much can we compress a file with English Wikipedia. 

There are more approaches to compress text files. Let's go through them and you can see in the next widget how they fare for various types of data. 

- _Baseline_: The stadard way to store text files is UTF-8. Lying a bit, this format store each letter using 8 bits. <Footnote>Why is it lying? [UTF-8](https://en.wikipedia.org/wiki/UTF-8) itself is a beautiful example of good engineering inspired by coding theory. There are around 100 characters (English letters, digits) that are stored using 8 bits, fancier letters from reasonable alphabets are stored using 16 bits, and emojis like ðŸ˜€ or hieroglyphs like ð“€€ take 32 bits. Classic coding theoryâ€”rare stuff gets longer codes. But English Wiki is mostly standard stuff, so 8 bits per letter it is. </Footnote>

- _Optimal letter-independent code_: We discussed above, how coding theory tells us what's the best way of compressing files if we don't want to use tricks like "'th' is usually followed by 'e'". We can treat the letters as independent and encode the using $H(p)$ bits per letter on average, where $p$ are the English letter frequencies. [In this widget above](/02-crossentropy#construction), we used those frequencies as an example; the entropy is a bit north of 4 bits. So, we can shrink the file almost by __2x__ just by using that different letters have different frequencies. 

- _Zipping_: Standard compression algorithms like those used in zip use codes, but also look for repeating patterns or take advantage of frequencies of letter pairs. They can compress English text up to a factor around __3x__.  

- _The best algorithms_ in Hutter's competition have compress by a factor of about __8x__. That's about 1 bit per letter.  

- _LLMs_: We know about algorithms that are arguably even better compressors - LLMs. Large Language Models are literally trained on being able to predict text - given a snippet of the text like "My name is A", LLM tries to predict the distribution $p$ of the next letter <Footnote>Well, token.</Footnote>. 

If we can predict the next letter of a text, this means you can also compress the text. The compression algorithm is this: Given text $s$ = "My name is A", we run LLM to guess the distribution $p_s$ of the next letter. Then, we use the best code for $p_s$ to store the actual next letter, say 'l'. 

If the actual next letter is 'l', then the surprisal of LLM upon seeing this letter is $\log 1/p('l')$. We can now save the letter 'l' to a file by using the best possible code encoding symbols with frequencies $p$; that is, we can store 'l' using $\log 1/p('l')$ bits of memory. The decoding algorithm is very similar, it runs the same LLM and always compute the next code $p_s$ used to decode the next letter. <Footnote>In practice, this gets more complicated whenever $\log 1/p('l')$ is not a whole number. We had the same issue in our coding theory discussion, let's not go into it. </Footnote>

The fun part (that we will understand better later on) is that LLMs are being trained to optimize the so-called cross-entropy loss - in a nutshell this means that if the next letter is 'l', we want the surprisal $\log 1/p('l')$ of the network to be as small as possible. Using our coding-theory intuitions - the surprisal is also the code length in the best code - we can view the training of LLMs as the training to compress English text as much as possible. 

[todo fill numbers]
It is thus maybe not so surprising that they are incredibly good at it! In the following experiments, GPT2 achieved similar compression to the winners of Hutter challenge and Llamma 3 was a good bit better. Wait a minute, how come then that the world record is 120MB and not ~50MB? The problem is that Hutter only wants you to compress 1GB of text, not the whole Internet. If you use GPT2 to compress a piece of text, then the size of the compressed file should also include the size of GPT2 itself. If Hutter's challenge was about compressing 1000GB-sized file, using LLMs would be a no-brainer, but, alas, Hutter created his challenge before the current AI paradigm based on working with _lots_ of data. 

Before playing with the widget, let me tell you about one of the coolest experiments I know of. Claude Shannon, who [invented information theory](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication) in the late 40s, did [the following](https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf) a few years later. He'd show people partial sentences and ask them to guess the next letter. This way, he figured that people can compress English to about 0.5 - 1 bits per letter, similarly to the GPT-2 performance. As far as I can say, this is the first experiment about next-token prediction; Claude Shannon was so based he did it 60+ years before it became cool! 

See compression factors of different types of text and different algorithms with the widget below. Notice how the compression ratio depends heavily on the structure and predictability of the text. <Footnote>Let me know what text I should add to the widget! </Footnote>

<CompressionWidget />

</Expand>
