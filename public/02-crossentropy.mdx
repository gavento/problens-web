# KL properties & (Cross-)entropy

In this chapter, we will discuss a few nice algebraic properties of KL divergence. Also, it's often very useful to split the formula into two terms called _cross-entropy_ and _entropy_. 


We will also go through a few basic properties that these things have.

## Properties of KL divergence

Let's go through several properties of KL divergence it's good to understand. Remember that KL divergence is algebraically defined like this:

$$
D(p,q) = \sum_{i = 1}^n p_i \log \frac{p_i}{q_i}
$$

KL divergence can be infinite: this happens whenever $q_i \not= 0$ and $p_i = 0$ for some $i$. We also use convention $0 \cdot \log 0/0 = 0$ so that KL is always a well-defined number or $+\infty$.

### Asymmetry

The KL formula is not symmetrical -- in general, we don't have $D(p,q) = D(q,p)$. This is sometimes seen as a drawback, especially when you compare KL with (symmetrical) distance function like the $\ell_1$ or $\ell_2$ metric<Footnote> <Math math='d_1(p,q) = \sum_{i = 1}^n |p_i - q_i|'/>, <Math math='d_2(p,q) = \sqrt{\sum_{i = 1}^n (p_i - q_i)^2}'/></Footnote>. I want to stress that the asymmetry is by design! KL measures how well a distribution $p$ is fitted by a model $q$. This is an asymmetrical notion, so we need an asymetrical formula, nothing to be ashamed for here.

In fact, the word _divergence_ is used for things like KL that behave kind of like distances but are not really symmetrical. <Footnote>More on that in [one of the last chapters](06-algorithms)</Footnote>

<Expand headline = "Example">
Imagine that the true probability distribution is 50\%/50\% (i.e., fair coin), and the model is $100\% / 0\%$. In that case, KL divergence is infinite. That's because if we flip a coin, we have 50\% chance of flipping tails, which the model distribution says should never happen. This means that we have 50\% chance of gaining infinitely many bits of information (whatever our prior on fair/biased was, the posterior should now be 100\% fair and 0\% biased) and hence the divergence is infinite.

On the other hand, if the truth is $100\% / 0\%$ and the model is $50\% / 50\%$, then $D(p,q) = 1$. That's because each flip, we are going to flip heads and thus gain one bit of evidence towards the coin being fair. One bit is a lot and our probability of the coin being fair goes down really fast, but it's never zero. We simply have to account for the (exponentially unlikely) event that even if the coin is fair, all flips may have happenned to be heads.
</Expand>

### Nonnegativity

If you plug in the same distribution twice into KL, you get

$$
D(p, p) = \sum_{i = 1}^n p_i \cdot \log \frac{p_i}{p_i} = 0
$$

because $\log 1 = 0$.
This makes sense, you can never distinguish the truth from the truth. ü§∑

Even better, KL divergence is always nonnegative. I think we got a pretty strong intuition why in the previous chapter. Just imagine that you have two distributions $p$ and $q$, but as you keep sampling from $p$, you gradually keep believing more and more that the distribuition actually is ... $q$? The world would be really, really messed up in that case! Feel free to check the formal proof below. 


<Expand headline="Proof of nonnegativity">
We will use the natural logarithm to make the proof a bit shorter, i.e., we want to prove that <Math math = "D(p,q) =  \sum_{i = 1}^n p_i \cdot \ln \frac{p_i}{q_i}  \ge 0" /> holds for any $p,q$. 

We will estimate the expression $\ln \frac{p_i}{q_i}$ inside the summation. Since we already know that the inequality is tight if all $p_i = q_i$, we should choose an estimate of logarithm which is tight around 1. The best approximation of logarithm around $1$ is $\ln (1+x) \le x$. We can use it like this:

$$
-D(p,q)
= \sum_{i = 1}^n p_i \cdot \ln \frac{q_i}{p_i}
\le \sum_{i = 1}^n p_i \cdot  \left( \frac{q_i}{p_i} - 1 \right)
= \sum_{i = 1}^n q_i - p_i
= 1 - 1 = 0
$$

</Expand>


A bit stronger version of this fact is also called monotonicity. We won't really use it, so feel free to skip it. 

<Expand headline="Monotonicity explanation">
Imagine that we have two distributions $p,q$, let's say they are over countries (\{üá∫üá∏,üá®üá¶,üá≤üáΩ,üáØüáµ,üá¨üáß,üáÆüá≥,...}). Then, we zoom out a little bit and consider a set of larger geographical areas (\{North America, Europe, ...}). We can convert $p,q$ to this rougher set to get distributions $p', q'$ -- e.g. we set <Math math = "p'(\textrm{North America}) = p(üá∫üá∏) + p(üá®üá¶) + p(üá≤üáΩ)" />. Then it's the case that

$$
D(p, q) \ge D(p', q')
$$

This makes sense! Remember that KL divergence tells us how hard it is to separate $p$ from $q$ using Bayes' theorem. This just says that for our Bayesian reasoner, it gets harder to distinguish $p$ from $q$, as we keep grouping different outcomes together. In the special case, we could group all the outcomes to a single outcome SOMETHING_HAPPENED -- the inequality would than reduce to $D(p,q) \ge 0$, i.e., nonnegativity is a special case of this.
</Expand>

### Additivity

Let's say that we have two distribution (the "truth" and the "model") $p, q$ and then another two distributions $p', q'$. It's true that

$$
D(p \otimes p', q \otimes q') = D(p, q) + D(p', q')
$$
where $p \otimes p'$ stands for the product distribution with marginals $p,p'$, i.e. the joint distribution where $p,p'$ are independent.

We have used this property implicitly in the previous chapter -- the property is really just saying that if you are computing the over KL divergence in the setup of flipping a coin many times, it's just sum of divergences for individual flips. 

In fact, a slightly stronger property called _chain rule_ is true. We won't really use it, so feel free to skip. 

<Expand headline="Chain rule explanation">
Imagine that we have two distributions $p,q$ for how I commute to work (\{üö∂‚Äç‚ôÄÔ∏è, üö≤, üöå}). Later, I decide that when I take bus, I will also keep track of what line I took (\{‚ë†, ‚ë°, ‚ë¢}); I again have two (so-called conditional) distributions $p', q'$ for that. Now, $p$ and $p'$ can be combined into an overall distribution <Math math = "p_{\textrm{overall}}" /> over the set \{üö∂‚Äç‚ôÄÔ∏è, üö≤, ‚ë†, ‚ë°, ‚ë¢}. For example, if $p=$\{üö∂‚Äç‚ôÄÔ∏è: 0.3, üö≤: 0.3, üöå: 0.2\} and $p'=$\{‚ë†: 0.5, ‚ë°: 0.25, ‚ë¢: 0.25\}, then <Math math = "p_{\textrm{overall}}=" />\{üö∂‚Äç‚ôÄÔ∏è: 0.3, üö≤: 0.3, üöå‚ë†: 0.1, üöå‚ë°: 0.05, üöå‚ë¢: 0.05\}. 

The chain rule property of KL divergence is the fact that

<Math displayMode={true} math="
D(p_{\textrm{overall}}, q_{\textrm{overall}}) = D(p,q) + p_{\textrm{bus}} \cdot D(p',q')
"/>
Here, <Math math = "p_{\text{bus}}" /> is the probability of üöå according to the (true) distribution $p$. 

This property makes sense! First, notice that distinguishing the truth $p_{overall}$ from the model $q_{overall}$ gets easier than distinguishing $p$ from $p$. But the formula even says how much -- the additional refinement of the option üöå  is helping us by $D(p', q')$ whenever it comes up, which happens with frequency <Math math = "p_{\textrm{bus}}" />. 

Try to derive this property or see how it implies additivity! 
</Expand>


### Uniqueness

As it turns out, any reasonable function that that has the monotonicity and chain rule properties already [has to be KL divergence](https://blog.alexalemi.com/kl.html).

This is pretty cool and it helps us to appreciate that KL divergence is not at all arbitrary function. There's just one measure with the natural properties of KL divergence -- KL divergence itself.


## Relative entropy = Cross-entropy - Entropy

First, a quick recap: In the [previous chapter](01-kl_intro), we've seen how KL divergence is really the result of repeatedly applying Bayes' theorem, using this table:

![Surprisal](01-kl_intro/repeated_bayes_surprisals.png)

Each cell in this table holds a value of type $\log 1/p$ called the surprisal.
Last time, we have been parsing this table "row-by-row". That is, we always subtracted two surprisals to compute how much evidence we got towards one of the two hypotheses. The Bayesian reasoner simply keeps summing up all those surprisal differences.

We can also try to understand the table "column-by-column". That is, the Bayesian reasoner can compute the total surprisal for both hypothesis, then convert those into the final posterior probability. This corresponds to rewriting the KL divergence as follows:

<Math displayMode={true} math='
\underbrace{\sum_{i = 1}^n p_i \log \frac{p_i}{q_i}}_{D(p,q)}
 =
\underbrace{\sum_{i = 1}^n p_i \log 1 / q_i}_{H(p,q)}
 -
\underbrace{\sum_{i = 1}^n p_i \log 1 / p_i}_{H(p)}
'/>

The two expressions on the right hand side are important. We will try to get some intuition about them now. 

### Cross-entropy: 
The term $H(p,q) = \sum_{i = 1}^n p_i \log 1 / q_i$ is called _cross-entropy_. You should think of this expression as telling you how surprised you are, on average, if you keep seeing data from $p$ and try to model them by $q$. 

For example (see the picture), let's say we keep flipping fair coin, so $p =$ \{H: 0.5, T: 0.5\}. If we use the same distribution as our model, i.e., $q =$ \{H: 0.5, T: 0.5\}, the cross-entropy is equal to 1. 

On the other hand, what if $q =$ \{H: 0.999, T: 0.001\}? Although we are not surprised to see heads at all, each time we see a tail, our surprisal is equal to $\log 1/0.001 \approx 10$. So, our average surprisal -- cross-entropy -- is about 5 which is a good bit bigger. 

![cross-entropy](02-crossentropy/crossentropy.png)

In general, better models are less surprised by the data, hence they have smaller cross-entropy. 

### Entropy: 

The term $H(p) = H(p, p) = \sum_{i = 1}^n p_i \log 1 / p_i$ is an important special case of cross-entropy called _entropy_. This is the best possible cross-entropy we can achieve for a distribution $p$ if we model it by $p$ itself.  

Intuitively, entropy is telling us how much suprise or uncertainty is inherent for $p$. For example, even if we know that we are flipping a fair coin, we still don't know which side it lands on, so there is something to learn about the outcome of the experiment. The entropy of the fair coin flip is <Math math = "H(\{\textrm{H: }1/2, \textrm{T: }1/2\}) = 1" />. 

The entropy can get much smaller -- for example,<Math math = "H(\{\textrm{H: }0.001, \textrm{T: }0.999\}) = 0.01" />. This makes sense! Although seeing heads generates the surprise of $\log 1/0.001 \approx 10$, this is simply too rare to change the _average_ surprise dominated by the fact that most flips are easily predictable. The entropy can go down all the way to zero, if the distribution has 100\% chance of giving one outcome. 

![entropy](02-crossentropy/entropy.png)

On the other hand, the entropy can also get much larger than 1 bit. For example, the entropy of throwing a die is equal to $\log_2(6) \approx 2.6$ bits and in general, the entropy of a uniform distribution over $k$ options is $\log_2 k$. The uniform distribution also has the largest entropy among distributions with $k$ options. This again makes sense -- there's most uncertainty if all the options are equally plausible.  

### Relative entropy: 
Finally, $D(p,q)$ is the difference between these two values. That is, it tells us how close our average surprisal measured by cross-entropy is to the ultimate limit measured by entropy. 
That's also why in some communities, KL divergence is known as the _relative entropy_ between $p$ and $q$. <Footnote>Much better name than KL divergence, if you ask me. The name KL divergence seems to be much more common, though, so we stick with it. </Footnote>

### So what is this good for?

Splitting a summation into two parts is not some kind of hard-core mathematical trick -- the main advantage of adding cross-entropy & entropy to our vocabulary is that they are simply very meaningful concepts. 
<Footnote>
In fact, entropy is probably a good bit more well-known than KL divergence. The reason why this mini-course builds everything up using KL divergence is related to the fact that I am trying to strike a balance between an excited stream-of-consciousness wall of text about underrated applications of probability, and something resembling a coherent text. 
</Footnote>

Let's see an example of this. 

<Expand headline = "Riddle: Evaluating Predictions"> <a id="application-evaluating-predictions"></a>

As the next application of KL divergence and cross-entropy, let's solve our [prediction riddle](00-introduction/predictions). Recall that we asked about a few experts to predict future events and recorded their probabilities -- how do we measure their performance?

### Idealized KL score

To grade an expert's predictions, let $p = (p_1, \dots, p_n)$ denote the true probabilities of the events happening and $q$ denote the expert's predicted probabilities. It sure sounds like a good idea to define the expert's score as:

$$
S_{KL}(q) = D(p,q)
$$

We will for simplicity assume that all the $n$ events are independent, where this simplifies to the following formula:

$$
S_{KL}(q) = \sum_{i = 1}^n  \left(
    p_i\log\frac{p_i}{q_i} + (1-p_i)\log\frac{1-p_i}{1-q_i}
    \right)
$$

{/*
For example, if we ask an expert about two independent events that happen with probabilities $30\%$ and 90\%, and she estimates the probabilities as $50\%$ and $80\%$, her score would be

$$
S_{KL}(q) = 0.3 \cdot \log 0.3/0.5 + 0.7 \cdot \log 0.7/0.5 \\
+ 0.9 \cdot \log 0.9 / 0.8 + 0.1 \cdot 0.1 / 0.2
\approx ??.
$$*/}

There's one big problem with our approach -- can you see it?

### Cross-entropy score

The problem is that when we are grading, we have absolutely no idea what the "true" probabilities are! <Footnote> We could now have a very long philosophical discussion about whether something like "true" probability even exists. Fortunately, this is not very relevant now. </Footnote>

The only fact we know is whether each event happenned or not. This also corresponds to a valid empirical probability distribution $\hat{p}$ -- it's just the case that $\hat{p}_i = 0$ or $\hat{p}_i = 1$ for each $i$, i.e., this empirical distribution is supported on a single outcome that we observed.

What happens if we plug $\hat{p}$ into our KL score from ?? instead of $p$? Since the entropy of $\hat{p}$ is zero (it's supported on a single outcome), the cross-entropy and relative entropy are the same, i.e., 

<Math displayMode={true} math="
S_{CE}(q) =
&\sum_{i = 1}^n  \left(
    \hat{p}_i\log\frac{\hat{p}_i}{q_i} + (1-\hat{p}_i)\log\frac{1-\hat{p}_i}{1-q_i}
    \right)\\
    &\sum_{i = 1}^n  \left(
    \hat{p}_i\log\frac{1}{q_i} + (1-\hat{p}_i)\log\frac{1}{1-q_i}
    \right)
"/>

This is why this kind of expression is usually called the _cross-entropy score_, although, as far as I can say, in the forecasting community it's typically called the [Log-score](https://forecasting.wiki/wiki/Log_score).

### Connection to the idealized score
Let's understand more deeply how the cross-entropy score connects to the idealized KL score. Technically speaking, cross-entropy score is a random variable because it depends on the randomness of $p$. More concretely, each $\hat{p}_i$ is actually a random variable equal to 1 with probability $p_i$, and otherwise equal to 0.

So what's the expected value of the cross-entropy score, i.e., $E_p[S_{CE}(q)]$? Well, since $\hat{p_i}$ is on average equal to $p_i$, i.e., $E_p[\hat{p}_i] = p_i$, we simply have by linearity of expectation that

<Math displayMode={true} math="
E_p[S_{CE}(q)] =
\sum_{i = 1}^n  \left(
    p_i\log\frac{1}{q_i} + (1-p_i)\log\frac{1}{1-q_i}
    \right)
"/>

That is, <Math  math="E_p[S_{CE}(q)] = H(p,q)"/>. This is pretty cool! If we give our experts many questions, then in the long run, we can use the law of large numbers to argue that their score is going to be close to the cross-entropy $H(p,q)$ between the "true" probability distribution $p$ and their estimate.

At this point, let's remember the equation

$$
D(p,q) = H(p,q) - H(p)
$$

Although we can't compute the value of $D(p,q)$, notice that for two expert predictions $q_1, q_2$, we have $D(p,q_1) < D(p, q_2)$ if and only if $H(p, q_1) < H(p, q_2)$: KL divergence and cross-entropy only differ by $H(p)$ which a constant independent of $q_1, q_2$. That is, _comparing the experts by their cross-entropy score is in the long run the same as comparing them by their KL score!_

<Expand headline="Example: Coin flipping">
Let's see an example of this. Let's say we flip a fair coin $N$ times ($p_1 = \dots = p_N = 1/2$) and ask two experts for their probabilities. Let's say that the first expert predicts correctly ($q_1 = \dots = q_N = 1/2$) and the other one is a bit off ($q'_1 = \dots = q'_N = 0.6$).

Then, the idealized KL score happens to be $KL(p, q) = N \cdot 0 = 0$ and $KL(p, q') = N \cdot D(p_1, q'_1) \approx 0.03 \cdot N$.

We don't have access to this score, but if we compute the experts' cross-entropy scores, the law of large numbers tells us that for large enough $N$, the scores are going to be roughly $H(p,q) = N$ and $H(p, q') \approx 1.03 \cdot N$. Both crossentropies are larger by the entropy of the underlying process which is $H(p) = N$. So, the score of both experts is quite high, even if the first expert is as good as it gets. However, since both values are shifted by the same amount, using the score to select the best experts is as good as using the ideal KL score in the long run. 

Notice that _"in the long run"_ is really important here. In our example of 8 predictions, computing their crossentropy (or any other score function) is premature. 
</Expand>
</Expand>

## Coding Theory Intuitions <a id="coding"></a>

One of the most powerful intuition pumps about entropy and related concepts comes from information theory, in particular coding theory. Let's explain it here very briefly.

If you look at Morse code, you might notice how the code for certain frequent letters like e or t are much shorter than the codes for rare letters like h or q.

To build the theory behind building codes like Morse's, let's imagine the following task. Consider the distribution over the English alphabet a,b,...,z where the probability of each letter is its approximate frequency in English text.

Our task is as follows. We should come up with some kind of binary encoding scheme so that if we get a long English text, represented as a large amount $n$ of independent samples from the above distribution, the number of bits we use is the least possible.

It turns out that the best possible solution to this problem satisfies that the average amount of bits we spend per letter -- called the rate of the code -- is $\sum p_i \log \frac{1}{p_i}$, the entropy of the distribution. For example, if the entropy of the distribution is about 4.17 bits, there is a code spending that many bits per letter on average, and there is no better code.

Instead of proving why the entropy is the answer to our question, let's see examples of (suboptimal) codes that suggest why the answer is entropy.

First, we could encode each letter using 5 bits -- a becomes 00000, b becomes 00001 and so on. The rate of this scheme is 5 bits per letter, and it can be improved quite a bit.

One way to improve it is to assign codes of different lengths to different letters, based on their frequency. The following turns out to be a good rule for the lengths: if a letter $i$ has frequency $p_i$, its code length should be about $\log \frac{1}{p_i}$.

As a specific example, if the alphabet were just a-d and the letter frequencies were $1/2, 1/4, 1/8, 1/8$, the best code is:

- a - 0
- b - 10
- c - 110
- d - 111

That is, "baba" is encoded as "100100".

You might notice how the letter with frequency $1/2^i$ has a code of length $i$; that is, frequency $p_i$ leads to a code of length $\log \frac{1}{p_i}$.

If all the probabilities in the distribution are $1/2^k$ for some number $k$, the generalization of the above code -- having $1/2^k$ probability letters have lengths $k$ -- is always possible.

If the probabilities in the distribution are not "round", you can't quite achieve a code mapping letters to binary strings as good as the entropy $H(p)$ of the distribution due to the rounding issues. You can only achieve a code of rate at most $1 + H(p)$. For example, the entropy of our English letter distribution is about $4.17$, but the best code mapping letters to bits (so-called Huffman code) has slightly worse rate of about $4.25$.

But that's not a big deal. If you allow codes that don't necessarily map single letters to bit strings, but that can also map pairs of letters and so on, then you can get codes with rate arbitrarily close to the entropy.

## Next Steps <a id="next-steps"></a>

We are getting comfortable with KL divergence! A quick recap:

![Formula for KL divergence](02-crossentropy/crossentropy_entropy_formula.png)

If it is really the right way to measure how well a model fits the truth, it sure seems that making it small is a good way to build good models!? [Let's see in the next chapter](03-minimizing)!



