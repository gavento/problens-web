# Loss functions

At last, the final chapter! We'll now leverage our newfound KL superpowers to tackle machine learning, or at least, understand a crucial aspect of it: [setting up loss functions](00-introduction/machine-learning). 

The core insight here is that **KL divergence guides us from a rough idea about important data aspects to a concrete estimation algorithm**. Specifically, we can use the _maximum entropy principle_ to transform our initial concept into a fully probabilistic model, and then apply _maximum likelihood_ to derive a loss function for optimization.

We will explore the examples in the following table (feel free to skip some) and illuminate the origins of their respective loss functions.

![Examples](06-machine_learning/table.png)

<KeyTakeaway>
Maximum entropy and maximum likelihood principles explain many machine-learning algorithms. 
</KeyTakeaway>

## üìö A Catalogue of Examples <a id="examples"></a>

<Expand headline="Estimating mean and variance"> <a id="application-to-statistics-riddle"></a>

Our [basic statistics riddle](00-introduction/statistics) posed the following: Given a set of numbers $X_1, \dots, X_n$, how do we estimate their mean and variance?

We've already approached this riddle from various angles; now, let's combine our insights.

First, we transform the general idea that mean and variance are important into a concrete probabilistic model. The [maximum entropy principle](04-max_entropy) suggests modeling the data as independent samples drawn from [the gaussian distribution](04-max_entropy#normal).

Once we have a set of possible models‚Äîall Gaussian distributions‚Äîwe can select the best one using [the maximum likelihood principle](03-minimizing#mle).

We want to find $\hat\mu, \hat\sigma^2$ that maximizes
<Math displayMode={true} math = "\hat\mu, \hat\sigma^2 = \argmin_{\mu, \sigma^2} \prod_{i = 1}^N \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(X_i-\mu)^2}{2\sigma^2}}"/>
It's typically easier to write down the logarithm of the likelihood function, i.e., the log-likelihood. This is not too surprising since [we understand](03-minimizing#mle) that maximizing log-likelihood is a synonym to minimizing KL divergence. If we make the problem simpler for us and consider only the estimation of the mean $\mu$, the equation simplifies like this:

<Math displayMode={true} math = "\hat\mu = \argmin_{\mu} \sum_{i = 1}^N (X_i-\mu)^2"/>

In this specific case, the optimization problem has a closed-form solution $\hat\mu = \frac{1}{N} \cdot \sum_{i = 1}^N X_i$ (and the formula for $\hat\sigma^2$ is analogous). Notice that while our formulas themselves don't explicitly mention probabilities, probabilities are essential for understanding the underlying mechanics.

What I want to emphasize is how our only initial assumption about the data was simply, "we have a bunch of numbers, and we care about their mean and variance." The rest flowed automatically from our understanding of KL divergence. 

</Expand>
<Expand headline="Linear regression"> <a id="linear-regression"></a>

Suppose we are given a list of pairs $(x_1, y_1), \dots, (x_n, y_n)$. We believe the data exhibits a roughly linear dependency, meaning there exist some constants $a,b$ such that $y_i \approx a\cdot x_i + b$. Our objective is to determine $a$ and $b$.

Let's transform this into a concrete probabilistic model. We'll model the data by assuming $y_i$ originates from the distribution $a\cdot x_i + b + \text{noise}$.<Footnote>We could also model the distribution from which $x_i$'s are drawn, but that won't be relevant for our current goal.</Footnote>

The noise is generated from a real-valued distribution. How do we choose it? The uniform distribution doesn't normalize over real numbers, making it unsuitable; the same applies to the exponential distribution. The next logical choice is the **Gaussian distribution** $N(\mu, \sigma^2)$, so let's select that. This introduces a slight complication as we now have two new parameters, $\mu, \sigma^2$, in our model $y_i \sim a\cdot x_i + b + N(\mu, \sigma^2)$, even though we primarily care about $a$ and $b$.

But this is fine. First, we can assume $\mu = 0$, because otherwise, we could replace $N(\mu, \sigma^2)$ with $N(0, \sigma^2)$ and $b$ with $b + \mu$ to achieve the same data model. We'll address $\sigma$ in a moment.

Let's proceed with our recipe and apply the maximum likelihood principle. We write down the likelihood of our data given our model:

<Math displayMode={true} math = "P((x_1, y_1), \dots, (x_n, y_n) | a, b, \sigma^2, x_1, \dots, x_n) = \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(ax_i + b - y_i)^2}{2\sigma^2}} " />

As usual, it's simpler to consider the log-likelihood (or cross-entropy):

<Math displayMode={true} math = "\log P(\text{data} | \text{parameters}) = -\frac{n}{2}\ln(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (a\cdot x_i + b - y_i)^2"/>

This expression appears rather complex, but we notice that for any fixed value of $\sigma^2$, the optimization problem for $a,b$ reduces to minimizing:

<Math displayMode = {true} math = "\hat{a}, \hat{b} = \argmin_{a,b} \sum_{i=1}^n (a\cdot x_i + b - y_i)^2"/>

Therefore, we can effectively disregard the added parameter $\sigma^2$<Footnote>Statistical software like R or Python's statsmodels also outputs $\sigma^2$. This can be helpful if you also, given some new point $x_{n+1}$, want not just to predict $y_{n+1}$, but also its error bars.</Footnote> and simply optimize above expression, which is the classical [least-squares loss function](https://en.wikipedia.org/wiki/Ordinary_least_squares) for linear regression.<Footnote>Typically, more variables $x^1, \dots, x^k$ are used to predict $y$. This leads to more complex formulas, but the derivation remains the same.</Footnote> Notice how the square in the loss function emerged directly from our maximum entropy assumption of Gaussian noise.

<Expand headline="Variants of linear regression">

Notice how our approach offers considerable flexibility, allowing us to incorporate additional information about the data into the model.

For example, perhaps we believe that the parameters $a$ and $b$ should be small. Our framework suggests augmenting the model by adding the prior $a,b \sim N(0, w^2)$, for some value $w$. This $w$ must be provided either by guessing or by employing another machine learning technique like [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)). This leads to optimizing $\sum_{i=1}^n (a\cdot x_i + b - y_i)^2 + \frac{1}{w^2}(a^2 + b^2)$, a method known as [Ridge regression](https://en.wikipedia.org/wiki/Ridge_regression).

As another illustration, consider the more general scenario where we have numerous predictors $x^1, \dots, x^k$, but we suspect that only a small fraction of them, say $0 < p < 1$, are genuinely useful. For the remaining predictors, the corresponding coefficient $a_i$ is zero. Our framework suggests that we should optimize:

<Math displayMode={true} math = "\min_{\substack{S \subseteq \{1, \dots, k\}, \\ |S|=pk}} \sum_{i=1}^n \left| \sum_{j \in S} a_j\cdot x_i^j - y_i\right|^2" />

The challenge is that optimizing this function is computationally difficult (in fact, NP-hard)‚Äîtesting all possible subsets of nonzero coefficients and fitting each one takes exponential time. However, more advanced mathematical insights demonstrate that optimizing a different objective, called Lasso, often effectively solves this complex problem.<Footnote>See e.g. [10.6 here](https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html) or [this book](https://link.springer.com/book/10.1007/978-3-642-20192-9)</Footnote> Therefore, you should consider **Lasso** as an attempt to solve our probabilistic model under the assumption that "only a fraction of variables are relevant."
</Expand>
</Expand>


<Expand headline="K-means clustering"> <a id="k-means-clustering"></a>

We are given a set of points $x_1, \dots, x_n$ on, say, a 2D plane. Our objective is to group them into $k$ clusters.

Let's transform this into a probabilistic model. We'll use $\mu_1, \dots, \mu_k$ to represent the centers of these clusters. The significance of these points is that if a point $x_i$ belongs to cluster $j$, then its Euclidean distance $||x_i - \mu_j||$ should be small.

As before, we can employ the maximum entropy distribution to construct a concrete model. Since the exponential function doesn't normalize, we will use the Normal distribution:

<Math displayMode={true} math = "p(x | \mu_j) \propto e^{-\frac{||x-\mu_j||^2}{2\sigma^2}}"/>

The notation $p(x | \mu_j)$ signifies that this is our model for points originating from the $j$-th cluster. However, we desire a complete probabilistic model $p(x)$. We can achieve this by assigning a prior probability to how likely each cluster is. The maximum entropy prior is the uniform distribution, so we will choose:

<Math displayMode={true} math = "p(x) = \frac{1}{k} \sum_{j = 1}^k p(x | \mu_j)"/>

We now have a probabilistic model that generates data $x$ from a distribution $p$. It's parameterized by $k+1$ values: $\mu_1, \dots, \mu_k$, and $\sigma^2$. We will use maximum likelihood to determine these parameters. The principle dictates that we should maximize the following log-likelihood:

<Math displayMode = {true} math = "\argmax_{\substack{\mu_1, \dots, \mu_k \\ \sigma^2}} -n \log \left( k\sqrt{2\pi\sigma^2}\right) + \sum_{i = 1}^n  \log \sum_{j = 1}^k e^{-\frac{||x_i-\mu_j||^2}{2\sigma^2}}"/>

Optimizing this expression corresponds to an algorithm known as [soft $k$-means](https://en.wikipedia.org/wiki/Fuzzy_clustering). The term "soft" indicates that the parameter $\sigma$ allows us to output a probability distribution for each point $x$, indicating its likelihood of belonging to each cluster.

In practice, people typically don't care that much about probabilistic assignment in $k$-means; knowing the closest cluster is usually sufficient. This corresponds to considering the limit as $\sigma \rightarrow 0$. In this limit, the messy expression above simplifies quite elegantly. Specifically, we can replace the summation <Math math = "\sum_{j = 1}^k e^{-\frac{||x_i-\mu_j||^2}{2\sigma^2}}" /> with <Math math = "\max_{j = 1}^k e^{-\frac{||x_i-\mu_j||^2}{2\sigma^2}}" /> because all terms in the sum, except the largest one, become negligible as $\sigma \rightarrow 0$. The expression then simplifies to:

<Math displayMode = {true} math = "\argmin_{\substack{\mu_1, \dots, \mu_k}} \sum_{i = 1}^n \min_{j = 1}^k ||x_i-\mu_j||^2"/>

The problem of finding $\mu_1, \dots, \mu_k$ that minimize this expression is called [$k$-means](https://en.wikipedia.org/wiki/K-means_clustering).
</Expand>



<Expand headline="Logistic regression"> <a id="logistic-regression"></a>

This time, we're presented with red and blue points on a plane, and our goal is to find the optimal line that separates them. Ideally, all red points would be on one side and all blue points on the other, but this isn't always achievable. In such cases, how do we determine the "best" separating line? <Footnote>Sometimes, people use the term "logistic regression" for 1D version of this where we have red and blue points on a line and want to split the line into two parts. Our algorithm works for any number of dimensions. </Footnote>

It will be easier to represent the line not as $y = ax+b$, but using its normal vector $\theta$ (orthogonal to the line) and the distance $\delta$ of the origin from the line, as shown in the image below.<Footnote>Specifically, $\theta = \frac{1}{a^2+1} (a, -1)$ and $\delta = |b|/\sqrt{a^2+1}$, but let's not get bogged down in those details.</Footnote>

![Logistic regression](06-machine_learning/logistic.png)

Given a point $(x, y)$, the crucial quantity is its distance from our line. This can be calculated using the dot product as $\theta \cdot (x, y) + \delta$.

Now, employing the maximum entropy principle, we construct a probabilistic model that transforms this distance into a probability of color. We utilize the [logistic function](04-max_entropy). That is, our model states:
<Math displayMode={true} math = "p(\textrm{red} | (x, y)) = \sigma\left( \lambda (\theta \cdot (x, y) + \delta) \right)"/>
where $\sigma$ is the logistic function $\sigma(x) = e^x / (1+e^x)$. Naturally, we also have <Math displayMode={false} math = "p(\textrm{blue} | (x, y)) = 1 - p(\textrm{red} | (x, y)). " />

The constant $\lambda$ is a new parameter that the max-entropy principle compels us to add to our probabilistic model. Fortunately, it's quite a useful parameter‚Äîit quantifies our confidence in the classification. This is convenient because if we want to classify a new point in the future, we can not only assign it a red/blue color based on which side of the line it falls, but also use the equation above to compute how certain we are about our classification.

Once we have a model, we can apply the maximum likelihood principle to find its parameters. This principle instructs us to find $a,b,\lambda$ that minimize the following log-likelihood:

<Math displayMode = {true} math = "\argmin_{\theta, \delta, \lambda} \sum_{i = 1}^n \ell_i \log \sigma(\lambda (\theta \cdot (x_i,y_i) + \delta)) + (1-\ell_i) \log (1-\sigma(\lambda (\theta \cdot (x_i,y_i) + \delta)))"/>

where $\ell_i$ is an indicator variable (i.e., $\ell_i \in \{0,1\}$) denoting whether the point $(x_i,y_i)$ is red. This problem, known as [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), is hard to solve exactly (NP-hard in particular), but gradient descent typically works well.
</Expand>



<Expand headline="Classification by neural networks"> <a id="neural-nets"></a>

We are provided with a vast collection of images, each assigned one of $k$ possible labels (e.g., a dog, a muffin). Our goal is to optimize a neural network that takes an image as input and outputs a probability distribution over these $k$ possible classes.

![dogs](06-machine_learning/dogs.png "taken from https://www.freecodecamp.org/news/chihuahua-or-muffin-my-search-for-the-best-computer-vision-api-cbda4d6b425d/")

Designing the architecture of a neural network is an extremely intricate problem that cannot be "solved" simply by name-dropping KL divergence. However, maximum entropy does offer some assistance: [we've already discussed](04-max_entropy#softmax) that the final layer of the network typically converts logits into probabilities using a softmax function. That is, if we build a neural network that transforms an input image $X$ into $k$ numbers $NN_1(X), \dots, NN_k(X)$, we should map them into probabilities as <Math math = "p_j(X) \propto e^{\lambda NN_j(X)}" />. The constant $\lambda$ can be optimized alongside the network's weights or simply hardcoded to $1$.

Next, the maximum likelihood principle states that we should maximize the log-likelihood. This means that if for an image $X_i$ with label $\ell_i$, the network outputs a distribution $p_1(X_i), \dots, p_k(X_i)$, we should aim to maximize:
<Math displayMode={true} math = "\argmax_{\substack{\textrm{neural net \\ weights}}} \sum_{i = 1}^n \log p_{\ell_i}(X_i)"/>

Maximizing the log-likelihood is equivalent to minimizing cross-entropy. Therefore, in this context, we usually refer to this as _training the network by minimizing the cross-entropy loss <Math math = "\sum_{i = 1}^n \log 1/p_{\ell_i}(X_i)" />._
</Expand>

<Expand advanced={true} headline="Variational autoencoders"> <a id="variational-autoencoders"></a>

Here's an observation relevant to the previous example as well. When we store images on a hard drive, we do so pixel by pixel, i.e., in the *pixel space*. However, in our brains, we store them very differently; perhaps we remember "it was a picture of a brown dog that looked like a muffin," or something similar. This corresponds to utilizing what's called the *latent space*. This is a hypothesized space where two images are considered "close" if they refer to similar objects. For instance, two images might be close in the latent space even if they are extremely far apart in the pixel space.

The ability to represent the latent space is crucial for any interesting work involving images. In fact, the reason we could separate dogs and muffins in the previous example was that the inner layers of the network were able to access concepts in the latent space, such as "are there ears in the picture."

As the next step, we now desire a system that can not only classify images but also generate new ones from scratch (think DALL-E or Midjourney) or do stuff like interpolating between two images *in the latent space* (see e.g. [this video](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/videos/interpolations-ffhq.mp4) that uses a different architecture than autoencoders). 

An autoencoder (see below) is a neural network architecture that can do this more interesting stuff. It processes an input image $x$ through an encoder, $\textrm{Enc}(x)$, compressing it into a smaller list of numbers $y$. Subsequently, a decoder, $\textrm{Dec}(y)$, attempts to reconstruct the original image $x$. The hope is that due to the bottleneck in the middle, $y$ captures the essential aspects of image $x$ while discarding less important ones. In other words, the encoder is hoped to encode the image into the latent space.

One challenge with traditional autoencoders is their tendency to learn a "hash function": The decoder might simply memorize all images $x_1, x_2, \dots$, and the encoder and decoder then decide on "names" for these images. The encoder then merely converts an input image to its name, and the decoder outputs the image associated with that name. To combat this issue, practitioners use variational autoencoders. These are autoencoders where the encoder outputs two lists: $\mu_1, \dots, \mu_d$ and $\sigma^2_1, \dots, \sigma^2_d$. The input to the decoder consists of $d$ samples drawn from $N(\mu_1, \sigma_1^2), \dots, N(\mu_d, \sigma_d^2)$. By introducing noise in the middle of the architecture, it becomes more difficult for the network to "cheat" by simply memorizing all images.

![vae](06-machine_learning/vae.png "image from https://en.wikipedia.org/wiki/Variational_autoencoder#/media/File:VAE_Basic.png")

The significant question now is: How do we optimize the variational autoencoder? Specifically, what is the loss function to minimize? Let's try to derive it! This will be more challenging than previous examples, hopefully demonstrating the depth of what we've learned in this minicourse!

Where do we begin? The first item on our to-do list is to formulate a probabilistic model of our data. In this example, we are interested in the joint distribution $p(x, y)$ over images‚Äîwhere $x$ represents the image in pixel space, and $y$ represents it in latent space. Notice that we are working with a probability distribution; a particular $y$ corresponds to an entire distribution over images that capture the concept of $y$, not just a single image.

Now, the encoder and decoder represent two distinct ways in which we can factorize the joint distribution.
The encoder corresponds to the factorization $p(x,y) = p(x) \cdot p(y | x)$: First, sample a random image and then encode it into the latent space. The distribution $p(x)$ is known to us: it's the empirical distribution over the large dataset of images we collected for training our autoencoder.
Unfortunately, the distribution $p(y | x)$ is not known, but the encoder attempts to represent it. More precisely, the distribution $p(y | x)$ is approximated by <Math math = "p'(y | x) = N(\textrm{Enc}_\mu(x), \textrm{Enc}_{\sigma^2}(x))" />.

The decoder provides us with a second way to model the data: We first sample a random latent-space representation and then decode it into an image, i.e., $p(x,y) = p(y) \cdot p(x|y)$. We need to be more creative to transform this idea into a concrete probabilistic model. For a start, we don't know the marginal distribution $p(y)$; so we will simply model it with a Gaussian distribution $q(y) = N(0, I)$ ($I$ represents the identity matrix). How should we model $p(x|y)$? Using $\textrm{Dec}(y)$ is a good starting point, but we should also introduce some randomness, so let's add Gaussian noise $N(0,1)$ to every pixel of the final image and model it as $q(x|y) = N(\textrm{Dec}(y), I)$. We have arrived at a different probabilistic model for $p(x,y)$, namely $p(x,y) = q(y) \cdot q(x|y)$.

We will optimize our variational autoencoder by minimizing the KL divergence between these two models of $p(x,y)$. When using KL divergence, we typically prefer the first parameter to represent the "truth" and the second to be a model of it. In this instance, both distributions are models that we will train in unison. However, the first model, $p(x,y) = p(x) \cdot p'(y|x)$, is closer to the truth since it incorporates the actual data $p(x)$, while the second model is primarily a collection of parameters we wish to optimize. Thus, we will train our network by minimizing:

<Math displayMode={true} math = "D\left( p(x) \cdot p'(y | x), q(y) \cdot q(x|y)\right)" />

Finishing the job now boils down to algebra that I hid in the following Expand box so as not to scare you right away. 
<Expand headline="Algebra part">
Let's call our KL loss function $\mathcal L$. We can decompose it into two parts, known as the **reconstruction loss** and the **regularization term**, as follows:
<Math displayMode = {true} math = "\mathcal L = \underbrace{\sum_{x,y} p(x) p'(y|x) \log \frac{p(x)}{q(x|y)}}_{\textrm{Reconstruction loss } \mathcal L_1} + \underbrace{\sum_{x,y} p(x) p'(y|x) \log \frac{p'(y|x)}{q(y)}}_{\textrm{Regularization term } \mathcal L_2} "/>

Let's analyze each term in turn, starting with the reconstruction loss.

### Reconstruction Loss
We can further expand the term $\mathcal L_1$ as:
<Math displayMode={true} math = "\mathcal L_1 = \sum_x p(x) \log p(x) - \sum_{x,y} p(x) p'(y|x) \log q(x|y)"/>
The first term is simply the entropy of the distribution $p(x)$. In our case, $p(x)$ is a uniform distribution over $N$ images $X_1, \dots, X_N$, so it equals $\log N$. In any event, this term is a constant independent of all the parameters we optimize, so we can disregard it. Thus, minimizing $\mathcal L_1$ amounts to minimizing the second term, which we will now interpret.

To do this, recall that $q(x|y)$ involves running the decoder on $y$ and adding Gaussian noise $N(0,1)$ to the result; we therefore have
<Math displayMode={true} math = "q(x | y) \propto e^{-\frac{\| x - \textrm{Dec}(y)\|^2}{2d}}"/>
and hence, up to an additive constant that does not affect our optimization problem, we have
<Math displayMode={true} math = "\log q(x | y) = -\frac{\| x - \textrm{Dec}(y)\|^2}{2d}"/>

So, if we were in the setup of a regular autoencoder where $p'(y|x)$ is a deterministic encoding $\textrm{Enc}(x)$, minimizing the term $\mathcal L_1$ would reduce to minimizing the mean square *reconstruction* loss:
<Math displayMode={true} math = "\frac{1}{N}\sum_{i = 1}^N \frac{\| X_i - \textrm{Dec}(\textrm{Enc}(X_i))\|^2}{2d}. " />

Our case is more complex; we need to minimize:
<Math displayMode={true} math = "\frac{1}{N}\sum_{i = 1}^N \sum_{y} p'(y|X_i) \frac{\| X_i - \textrm{Dec}(y)\|^2}{2d}. " />

The term $p'(y|x)$, which samples from <Math math = "N(\textrm{Enc}_\mu(x), \textrm{Enc}_{\sigma^2}(x))" /> instead of being a deterministic encoding, is problematic to compute directly. We will discuss how to estimate this part of the loss function at the end.

### Regularization Term

Let's analyze the term <Math math = "\mathcal L_2 = \sum_{x,y} p(x) p'(y|x) \log \frac{p'(y|x)}{q(y)}" />. Remember, $p(x)$ is simply a uniform distribution over our image dataset, so we can rewrite this as:
<Math displayMode={true} math = "\mathcal L_2 = \sum_{i = 1}^N \frac{1}{N} \sum_y p'(y|X_i) \log \frac{p'(y|X_i)}{q(y)} = \sum_{i = 1}^N \frac{1}{N} D(p'(y|X_i), q(y))"/>
Thus, minimizing this term reduces to minimizing the sum of KL divergences between $p'(y|X_i)$ and $q(y)$ across our dataset $X_1, \dots, X_N$.

The first distribution $p'(y |X_i)$ is simply a Gaussian with mean $\textrm{Enc}_\mu(X_i)$ and variance that is a diagonal matrix with entries <Math math = "\textrm{Enc}_{\sigma^2}(X_i)" />. The second distribution $q(y)$ is precisely the Gaussian $N(0,I)$. There's a [simple formula](https://leenashekhar.github.io/2019-01-30-KL-Divergence/) for the KL divergence between two Gaussians that looks like this:
<Math displayMode = {true} math = "D(N(\mu, \sigma^2), N(0, I)) = \frac12 \sum_{j = 1}^d \left( \mu_j^2 + \sigma_j^2 - 1 - \log\sigma_j^2\right) "/>

Plugging this into $\mathcal L_2$, we find that we need to minimize the expression:
<Math displayMode = {true} math = "\frac{1}{N} \sum_{i = 1}^N \left( \frac12 \sum_{j = 1}^d \textrm{Enc}_{\mu, j}(X_i)^2 + \textrm{Enc}_{\sigma^2, j}(X_i) - \log \textrm{Enc}_{\sigma^2, j}(X_i) \right)"/>
This is called the regularization term since it ensures that our variational autoencoder can't simply degenerate into a vanilla autoencoder by setting $\sigma_i^2 = 0$.
</Expand>

After performing the algebra, we arrive at the following loss function:
<Math displayMode={true} math = "\frac{1}{N} \sum_{i = 1}^N \left( \sum_y p'(y | X_i) \frac{\| X_i - \textrm{Dec}(y)\|^2}{2d} \,+\, \left( \frac12 \sum_{j = 1}^d \textrm{Enc}_{\mu, j}(X_i)^2 + \textrm{Enc}_{\sigma^2, j}(X_i) - \log \textrm{Enc}_{\sigma^2, j}(X_i) \right)\right)"/>

In general, computing the expression <Math math = "\sum_{i = 1}^N \sum_y p'(y | X_i) \frac{\| X_i - \textrm{Dec}(y)\|^2}{2d}"/> is super hard (NP-hard), as it would require iterating over all the whole range of $p'(y|X_i)$. In practice, this expression can be estimated using the Monte Carlo method. 
That is, for each image $X_i$, we simply sample a few times from $p'(y|X_i)$ and compute the reconstruction loss $\| X_i - \textrm{Dec}(y)\|^2$. We estimate the expectation $E_y$ by plugging in these few samples. 
</Expand>

You are the master of KL, congrats! 

If you want to know even more, click on the Danger button in the menu. It reveals bonus riddles & chapters. Also, it reveals some advanced Expand blocks in the chapters you already read. All bonus content is marked with ‚ö†Ô∏è. Also, check out [Resources](../resources) and leave us feedback at the [About](../about) page.


TODO Add FREE intelligence test widget. 


## üß† Human vs AI: Next Letter Prediction

Inspired by Shannon's original experiment, you can now test your own next-letter prediction abilities against modern language models! The widget below shows you partial sentences from Wikipedia and asks you to guess the missing letter. Your score is based on how many attempts it takes you to find the correct letter.

<LetterPredictionWidget />

